# Кластеризация текстов с помощью нейросетевых представлений

## Дневник

| Кратко | Длинно | Ссылки |
|--------|--------|--------|
|Обзорные статьи алгоритмов кластеризации. Narrow and deep.| **@ Narrow**: A comprehensive survey of clustering algorithms: State-of-the-art machine learning applications, taxonomy, challenges, and future research prospects --- хорошая обзорная статья по обычным кластеризатором. Из полезного перечисленны (скорее всего) все существующие метрики качества. <br/> **@ Deep**: A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and Future Directions --- аналогичная статья только о глубокой кластеризации, хорошая таксономия, хорошие сслыки на кластеризации в отдельный доменах (текст в том числе)| [link1](https://www.sciencedirect.com/science/article/abs/pii/S095219762200046X?via%3Dihub) <br/> [link2](https://arxiv.org/abs/2206.07579)|
| Метрика для оценивания совершенно разных кластеризаторов --- с пересечением кластеров, иерархических, обычных| Element-centric clustering comparison unifies overlaps and hierarchy --- для любой кластеризации строим матрицу смежности (на основе информации о соседствах двух элементов в разных кластерах), и сравниваем между собой матрицы двух реализаций кластеризации. Есть открытая реализация |  [link1](https://arxiv.org/pdf/1706.06136.pdf) |
| BERTopic --- универсальный фреймворк для итеративной кластеризации | Хороший (>4k звезд) открытый фреймворк для итеративной (ембеддинги -> понижение размерности -> кластеризация). Любой модуль можно заменить на свой. Интеграция с SBERT. | [link1](https://github.com/MaartenGr/BERTopic) |
| Новый (2022г) глубокий кластеризатор с автоподбором k | DeepDPM: Deep Clustering With an Unknown Number of Clusters --- кластеризатор на основне итеративного обучения DPM под "надзором" сетки. Есть цитаты. Есть реализация | [link1](https://arxiv.org/abs/2203.14309) <br/> [link2](https://arxiv.org/abs/2203.14309) |
| Создатель SBERT говорит, что эмбеддинги GPT-3 не очень... | Была идея для сравнения с эмбеддингами SBERT'a попробовать эмбеддинги от OpenAI. Но данная статья на medium говорит, что даже пробовать не стоит... На конец 2021 - сердину 2022г эмбеддинги GPT-3 очень не очень. Идея в подвешенном состоянии... | [link1](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9) |
| Обзор датасетов (постарался набрать как можно более необычных датасетов) | Собирал датасеты с Kaggle. Старался подобрать максимально разные по своей природе и области датасеты, с разной длиной текстов, количеством классов. <br/> **@ Short News** --- выжимка из новости + заголовок новости + тип новости <br/> **@ Spotify Songs** --- скрипт песни + исполнитель <br/> **@ Amazon 20k** --- Отзывы на товары с Amazon + иерархическая (глубина иерархии = 3) категоризация товара <br/> **@ Software** --- официальные требования к свойствам софта + метки за что каждое требование отвечает (начежность, легальность и тд) <br/> **@ IMDb** --- Краткое описание фильма + жанр фильма <br/> **@ Emotion** --- эмоциональная окраска предложения + 8 разный эмоций |  **TODO** |
| TriMap --- (возможно) достойный соперник UMAP'у | TriMap: Large-scale Dimensionality Reduction Using Triplets ---  Понижение размерности основанное на триплетах, есть цитаты, есть реализация | [Link1](https://github.com/eamid/trimap) |
| Топологические dim reducers | LEARNING TOPOLOGY-PRESERVING DATA REPRESENTATIONS --- новые метод понижения размерности (АЕ с очень хитрным лоссом топологическим). Также есть сравнения с остальными методами, попробую что-нибудь завести (кода не очень хороший) | [link1](https://arxiv.org/pdf/2302.00136.pdf) |


